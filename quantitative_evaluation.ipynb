{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Quantitative Evaluation***\n",
    "\n",
    "This notebook shows the process of performing quantitative evaluation on the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Import packages***\n",
    "\n",
    "Before we begin, let's import all the necessary packages for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as s_metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set plotting theme\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Read data***\n",
    "\n",
    "Next, let's read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the results csv\n",
    "results = pd.read_csv(\"results.csv\")\n",
    "\n",
    "# Read data\n",
    "hate_target_groups = results[~results.hate_target_label.isna(\n",
    ")].hate_target_label.unique()\n",
    "hate_target_groups = np.append(hate_target_groups, \"other\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Get precision, recall and f1-score***\n",
    "\n",
    "Next, let's get precision, recall and f1-score for hate detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each prompt type\n",
    "for prompt_type in [\"Zero\", \"One\", \"Few\"]:\n",
    "  # Get metrics\n",
    "  precision = round(\n",
    "      s_metrics.precision_score(\n",
    "          results[\"hate_label\"], results[f\"{prompt_type}-Shot Hate Detection\"], pos_label=\"yes\"\n",
    "      ), 3\n",
    "  )\n",
    "  recall = round(\n",
    "      s_metrics.recall_score(\n",
    "          results[\"hate_label\"], results[f\"{prompt_type}-Shot Hate Detection\"], zero_division=0, pos_label=\"yes\"\n",
    "      ), 3\n",
    "  )\n",
    "  f1 = round(\n",
    "      s_metrics.f1_score(\n",
    "          results[\"hate_label\"], results[f\"{prompt_type}-Shot Hate Detection\"], pos_label=\"yes\"\n",
    "      ), 3\n",
    "  )\n",
    "  # Print metrics\n",
    "  print(\n",
    "      f\"{prompt_type}-Shot: precision({precision}), recall({recall}), F1({f1})\",\n",
    "      end=\"\\n\\n\"\n",
    "  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get precision, recall and f1-score for hate target detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only hate records\n",
    "hate_results = results[results.hate_label == \"yes\"]\n",
    "# Loop over each prompt type\n",
    "for prompt_type in [\"Zero\", \"One\", \"Few\"]:\n",
    "  # Get metrics\n",
    "  precision = round(\n",
    "      s_metrics.precision_score(\n",
    "          hate_results[\"hate_target_label\"],\n",
    "          hate_results[f\"{prompt_type}-Shot Hate Target Detection\"],\n",
    "          average=\"micro\"\n",
    "      ), 3\n",
    "  )\n",
    "  recall = round(\n",
    "      s_metrics.recall_score(\n",
    "          hate_results[\"hate_target_label\"],\n",
    "          hate_results[f\"{prompt_type}-Shot Hate Target Detection\"],\n",
    "          average=\"micro\", zero_division=0\n",
    "      ), 3\n",
    "  )\n",
    "  f1 = round(\n",
    "      s_metrics.f1_score(\n",
    "          hate_results[\"hate_target_label\"],\n",
    "          hate_results[f\"{prompt_type}-Shot Hate Target Detection\"],\n",
    "          average=\"micro\"\n",
    "      ), 3\n",
    "  )\n",
    "  print(\n",
    "      f\"{prompt_type}-Shot: precision({precision}), recall({recall}), F1({f1})\",\n",
    "      end=\"\\n\\n\"\n",
    "  )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Plot Confusion matrices***\n",
    "\n",
    "Next, let's plot confusion matrices for hate detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define confusion matrix labels\n",
    "labels = [\"yes\", \"no\"]\n",
    "\n",
    "# Loop over each prompt type\n",
    "for prompt_type in [\"Zero\", \"One\", \"Few\"]:\n",
    "  # Plot confusion matrix\n",
    "  col = f\"{prompt_type}-Shot Hate Detection\"\n",
    "  cm = s_metrics.confusion_matrix(\n",
    "      results[\"hate_label\"], results[col], labels=labels\n",
    "  )\n",
    "  ax = sns.heatmap(\n",
    "      cm, annot=True, linewidths=0.5, annot_kws={\"size\": 22},\n",
    "      xticklabels=labels, yticklabels=labels, vmin=0, vmax=36\n",
    "  )\n",
    "  ax.set_ylabel(\"True Label\", fontsize=22)\n",
    "  ax.set_xlabel(\"Predicted Label\", fontsize=22)\n",
    "  ax.set_title(col, fontsize=24)\n",
    "  cbar = ax.collections[0].colorbar\n",
    "  cbar.ax.tick_params(labelsize=22)\n",
    "  plt.xticks(fontsize=22, rotation=90)\n",
    "  plt.yticks(fontsize=22, rotation=0)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot confusion matrices for hate target detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only hate records\n",
    "hate_results = results[results.hate_label == \"yes\"]\n",
    "# Define confusion matrix labels\n",
    "labels = hate_target_groups\n",
    "# Loop over each prompt type\n",
    "for prompt_type in [\"Zero\", \"One\", \"Few\"]:\n",
    "  # Plot confusion matrix\n",
    "  col = f\"{prompt_type}-Shot Hate Target Detection\"\n",
    "  cm = s_metrics.confusion_matrix(\n",
    "      hate_results[\"hate_target_label\"], hate_results[col], labels=labels\n",
    "  )\n",
    "  ax = sns.heatmap(\n",
    "      cm, annot=True, linewidths=0.5, annot_kws={\"size\": 22}, vmin=0, vmax=4,\n",
    "      xticklabels=labels, yticklabels=labels\n",
    "  )\n",
    "  ax.set_ylabel(\"True Label\", fontsize=22, labelpad=20)\n",
    "  ax.set_xlabel(\"Predicted Label\", fontsize=22, labelpad=20)\n",
    "  ax.set_title(col, fontsize=24, pad=20)\n",
    "  cbar = ax.collections[0].colorbar\n",
    "  cbar.ax.tick_params(labelsize=22)\n",
    "  plt.xticks(fontsize=22, rotation=90)\n",
    "  plt.yticks(fontsize=22, rotation=0)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Implicit vs Explicit***\n",
    "\n",
    "Next, let's plot explicit vs implicit hate detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all implicit hate results\n",
    "hate_implicit_results = results[(results.hate_label == \"yes\") & (results.implicit_hate == \"yes\")].copy()\n",
    "\n",
    "# Loop over each prompt type\n",
    "for prompt_type in [\"Zero\", \"One\", \"Few\"]:\n",
    "  # Plot confusion matrix\n",
    "  col = f\"{prompt_type}-Shot Hate Detection\"\n",
    "  # Get match and mismatch\n",
    "  match_count = (hate_implicit_results[col] == hate_implicit_results[\"hate_label\"]).sum()\n",
    "  mismatch_count = (hate_implicit_results[col] != hate_implicit_results[\"hate_label\"]).sum()\n",
    "  # Plot count plot\n",
    "  sns.barplot(y=[mismatch_count, match_count], x=[\"Mistake\", \"Correct\"])\n",
    "  plt.title(f\"{prompt_type} Hate Detection Implicit Text\")\n",
    "  plt.ylabel(\"\")\n",
    "  plt.yticks([])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all implicit hate results\n",
    "hate_implicit_results = results[(results.hate_label == \"yes\") & (results.implicit_hate == \"yes\")].copy()\n",
    "\n",
    "# Loop over each prompt type\n",
    "for prompt_type in [\"Zero\", \"One\", \"Few\"]:\n",
    "  # Plot confusion matrix\n",
    "  col = f\"{prompt_type}-Shot Hate Target Detection\"\n",
    "  # Get match and mismatch\n",
    "  match_count = (hate_implicit_results[col] == hate_implicit_results[\"hate_target_label\"]).sum()\n",
    "  mismatch_count = (hate_implicit_results[col] != hate_implicit_results[\"hate_target_label\"]).sum()\n",
    "  # Plot count plot\n",
    "  sns.barplot(y=[mismatch_count, match_count], x=[\"Mistake\", \"Correct\"])\n",
    "  plt.title(f\"{prompt_type} Hate Detection Implicit Text\")\n",
    "  plt.ylabel(\"\")\n",
    "  plt.yticks([])\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b4f15494c1589fe1cc0fb528e47ce04d963bc216e233210b9f03d2a4efbf08c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
